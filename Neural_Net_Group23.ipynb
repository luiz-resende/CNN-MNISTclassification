{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Net_Group23.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Z8I9245aitXM"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8aog2l2NIrp"
      },
      "source": [
        "# COMP551 - MiniProject 3\n",
        "\n",
        "## Convolutional Neural Network with 5 blocks of Hidden Layers based on VGGNet architecture\n",
        "\n",
        "#### @author: Luiz Resende Silva"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aE_ppP2DNIrs"
      },
      "source": [
        "Enabling kaggle API and creating ```.kaggle``` directory with the ```kaggle.jason``` file to download datasets directly from competition API inside ```/content``` folder. Also enabling the download of ```.py``` file containing the functions and neural network class designed by the author.\n",
        "\n",
        "#### **WARNING**: the cell below MUST be RUN ONCE before running all the cells (Ctrl+F9) or RUN TWICE if each cell is going to be run individually (Ctrl+Enter), in order to enable the project to recognize the existence of the ```/.kaggle``` directory  and allow both the data and the ```project03functions.py``` to be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A674E_GJNIrt",
        "colab": {}
      },
      "source": [
        "# !pip install kaggle\n",
        "!mkdir .kaggle\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9IIhWOS5NIrx"
      },
      "source": [
        "**Downloading the author's script containing functions and CNN class**; and saving in ```/content/project03functions.py```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CoH7MYA2NIry",
        "colab": {}
      },
      "source": [
        "!kaggle kernels pull luizresende/project03functions -p /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUQKcIADW1BK",
        "colab_type": "text"
      },
      "source": [
        "**Downloading the README file** and **pre-trained model**; saving in ```/content/READEME.md```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK3Z_F-LJRs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download -d luizresende/readmeproject03ml -p /content\n",
        "!unzip readmeproject03ml.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZZkkLnzBNIr2"
      },
      "source": [
        "**Downloading the dataset** directly from kaggle competition and unzipping in the ```/content``` folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Fy9vcbRNIr3",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c modified-mnist -p /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yn9Q39qVNIr6",
        "colab": {}
      },
      "source": [
        "!unzip train_max_x.zip\n",
        "!unzip test_max_x.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f2S6_Q3POhG",
        "colab_type": "text"
      },
      "source": [
        "#### Importing modules and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr90gPxXLgJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################################################################################################\n",
        "'''                                           IMPORTING GENERAL LIBRARIES                                                '''\n",
        "############################################################################################################################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import seaborn as sb #Graphical plotting library\n",
        "import matplotlib.pyplot as plt #Graphical plotting library\n",
        "import pickle as pkl #Pickle format library\n",
        "import time #Library to access time and assess running performance of the NN\n",
        "import random #Generate random numbers\n",
        "import pdb #Library to create breakpoints\n",
        "\n",
        "from scipy.sparse import hstack #Scipy sparse matrix concatanation module\n",
        "############################################################################################################################\n",
        "'''                                      IMPORT SCIKIT-LEARN PREPROCESSING MODULES                                       '''\n",
        "############################################################################################################################\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "############################################################################################################################\n",
        "'''                                         IMPORT PYTORCH MODULES/LIBRARY                                               '''\n",
        "############################################################################################################################\n",
        "import torch as th\n",
        "import torchvision as tv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as nf\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "############################################################################################################################\n",
        "'''                                   PY FILE CONTAINING FUNCTIONS & CLASSES BUILT                                       '''\n",
        "############################################################################################################################\n",
        "import project03functions as pf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-D2wGaSDNIsA"
      },
      "source": [
        "#### **BEGINNING OF THE SCRIPT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUe7do-KQe7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################################################################################################\n",
        "'''                                                     BEGINNING OF THE SCRIPT                                                '''\n",
        "##################################################################################################################################\n",
        "\n",
        "###         DESCRIBING FILES' NAMES/PATHS          ###\n",
        "FileTrainImages = \"train_max_x\"\n",
        "FileTrainLabels = \"train_max_y.csv\"\n",
        "FileTestImages = \"test_max_x\"\n",
        "\n",
        "###                  READING FILES                 ###\n",
        "train_images = pd.read_pickle(FileTrainImages)\n",
        "train_labels = pf.Read_File_DF(FileTrainLabels, separation=\",\", head=0, replace=[], drop=False)\n",
        "Test_Images = pd.read_pickle(FileTestImages)\n",
        "\n",
        "###        PLOTTING DISTRIBUTION OF LABELS         ###\n",
        "train_labels['Label'].hist(bins=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLAXu5qh6HXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###       SAMPLE IMAGE FROM TRAINING DATASET       ###\n",
        "pf.View_Image(Matrix=train_images[(random.randint(0,1000)),:,:], Is_NumPy=True, Is_DF=False, Multiple=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc-h8vA5n-ei",
        "colab_type": "text"
      },
      "source": [
        "**Splitting** the entire training dataset into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwa1pIyMn87Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###     SPLITTING DATASET IN TRAIN-VALIDATION      ###\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_images, train_labels, test_size=0.10, random_state=10657,\n",
        "                                                    shuffle=True, stratify=train_labels['Label'])\n",
        "\n",
        "sub_sample = False #Flag to take only a subset to speed training process during tests\n",
        "\n",
        "if(sub_sample==True):\n",
        "    tra = 20000 #Defining number of training samples\n",
        "    val = 5000 #Defining number of validation samples\n",
        "    X_train, X_valid, y_train, y_valid = X_train[0:tra,:,:], X_valid[0:val,:,:], y_train.iloc[0:tra,:], y_valid.iloc[0:val,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANwqXXqNLXls",
        "colab_type": "text"
      },
      "source": [
        "#### Entering some **general parameters** that will be **used in the CNN and in some preprocessing steps**\n",
        "\n",
        "**PARAMETERS MUST BE SET**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kepCjKsOLX9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###       PARAMETERS FOR THE TRANSFORMATIONS       ###\n",
        "threshold = 200 #Setting the pixel grey/black intensity threshold. Any value below will be set to zero and clear the background image, with only the numbers remaining\n",
        "input_size = 128 # Input dimension in number of pixels\n",
        "output_size = 10 #Dimension of the output generated. This relates to the number of classes in this problem: numbers from 0 to 9\n",
        "batchs = 25 #The batch size used during training phase"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39fhV6wHRebp",
        "colab_type": "text"
      },
      "source": [
        "#### Performing **thresholding to clear the images' background** and retain only pixels for the numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLHJBzElRr9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###       THRESHOLDING IMAGES       ###\n",
        "\n",
        "do_thresholding = False #Flag to perform or not the image thresholding\n",
        "\n",
        "if(do_thresholding==True):\n",
        "    X_train = pf.Image_Thresholding(Matrix=X_train, threshold_px=threshold)\n",
        "    X_valid = pf.Image_Thresholding(Matrix=X_valid, threshold_px=threshold)\n",
        "    \n",
        "    Test_Images = pf.Image_Thresholding(Matrix=Test_Images, threshold_px=threshold)\n",
        "\n",
        "    print(\"Image thresholding performed!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXbJtXTS6Xuf",
        "colab_type": "text"
      },
      "source": [
        "#### Performing **normalization of pixel values** to to have their intensity in a scale from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMWcb46J6T8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###     NORMALIZING PIXEL VALUES     ###\n",
        "\n",
        "do_normalize = False #Flag to perform or not the image thresholding\n",
        "\n",
        "if(do_normalize==True):\n",
        "    X_train = pf.Image_Normalization(Matrix=X_train) #Dividing all pixels by the largest value and scaling their value\n",
        "    X_valid = pf.Image_Normalization(Matrix=X_valid)\n",
        "\n",
        "    Test_Images = pf.Image_Normalization(Matrix=Test_Images) \n",
        "    \n",
        "    print(\"Pixel normalization performed!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRFl2IysXEK4",
        "colab_type": "text"
      },
      "source": [
        "#### The data (input features and labels) is **converted to PyToch tensors**\n",
        "\n",
        "*P.S.1: variables are being overwriten at all steps to cause the less footprint as possible in the available RAM*\n",
        "\n",
        "*P.S.2: Reshaping of variables and One-Hot encoding of the labels can be done by changing the Boolean in the ```if``` statments*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLU7TI-eXJVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###       CONVERTING DATA TO PYTORCH TENSORS       ###\n",
        "X_train = th.from_numpy(X_train).float() #The functions in the CNN construction require that the input features are of the type float. Same for validation and test sets\n",
        "X_valid = th.from_numpy(X_valid).float() \n",
        "Test_Images = th.from_numpy(Test_Images).float() \n",
        "\n",
        "y_train = th.from_numpy(y_train['Label'].to_numpy()).long() #They also require that the input labels are of the type long. Same for validation set\n",
        "y_valid = th.from_numpy(y_valid['Label'].to_numpy()).long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vbMwaGYJs3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###      DOING ONE-HOT ENCODING OF THE LABELS      ###\n",
        "if(False): #Set this to True to perform One-Hot encoding of the lebels\n",
        "  y_train = th.from_numpy(pf.OneHotEncoder(y_train['Label'].to_numpy()))\n",
        "  y_valid = th.from_numpy(pf.OneHotEncoder(y_valid['Label'].to_numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb_iTX_2N_1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_valid.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo1vtZgiXjHJ",
        "colab_type": "text"
      },
      "source": [
        "#### Creating ```training``` and ```validation``` *datasets* and *loaders* using ```torch.utils.data.TensorDataset``` and ```torch.utils.data.DataLoader```. These will be fed to the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8BmjKHPXjh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = th.utils.data.TensorDataset(X_train, y_train) #Creating training dataset\n",
        "train_loader = th.utils.data.DataLoader(train_dataset, batch_size=batchs, shuffle= True) #Creating training dataloader with the train_dataset and the batch size specified\n",
        "\n",
        "valid_dataset = th.utils.data.TensorDataset(X_valid, y_valid) #Creating validation dataset\n",
        "valid_loader = th.utils.data.DataLoader(valid_dataset, batch_size=batchs, shuffle= True) #Creating validation dataloader with the valid_dataset and the batch size specified"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-wZDNxiY1Oi",
        "colab_type": "text"
      },
      "source": [
        "####  **Instantiating the neural network classes**: the convolutional neural network ```ConvNN_G23_Std``` created (or one of its variants) or the feed-forward neural network ```FFNN_G23``` as ```net```.\n",
        "\n",
        "The classes requires two parameters:\n",
        "1.   ```num-classes```, which is the number of classes in the classification problem (output dimension);\n",
        "\n",
        "2.   ```input_ratio```, which is the value used in the reshaping of the vector fed to the first fully (FC1) connected layer, where this number will depend on the resolution of the input images (matrix size) and the number of max pooling layers used in the class to match the required input size for the FC1 (e.g. for the current set, the output from the last convolutinal layer for the class ```CNN_G23_Std``` will be a tensor of shape ```([512,8,8])``` and the output of FC1 will be 512, such that the tensor must be reshaped to ```([-1, 512*8*8])```). This number is defined by the max pooling layers and can be discovered in the model summary, by uncommenting and running the command below the CNN instantiation. **In the FFNN_G23**, this parameters refers to the size of the image (e.g. in the current dataset, this value is 128).\n",
        "\n",
        "***OBS.1: the flag*** ```Is_CNN``` ***must be set to*** ```True``` ***if one of the convolutional neural networks is being istantiated or to*** ```False``` ***if the class*** ```FFNN_G23``` ***is being instantiated***\n",
        "\n",
        "***OBS.2: the complete archtecture of the neural network classes is described in the report. Please, refer to it of visual aid or to the*** ```README.md``` ***file (uploaded to this .ipynb file)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc5Xmb0oZPCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Is_CNN = True #Flag to choose which neural network to instantiate - this flag is passed on to other function in the training and accuracy steps,\n",
        "                #since they must expect different inputs from different NN types\n",
        "\n",
        "if(Is_CNN==True):\n",
        "    expected_dim = 8\n",
        "    net = pf.ConvNN_G23_Std(num_classes=output_size, input_ratio=expected_dim, soft_max=False, drop_out=False, drop_prob=0.25, FC4_relu=True)\n",
        "else:\n",
        "    net = pf.FFNN_G23(num_classes=output_size, input_ratio=128, soft_max=False, drop_out=False, drop_prob=0.25, final_relu=False)\n",
        "\n",
        "go_cuda = True #Change this if statment to False to avoid moving network to GPU - This flag will be passed on to other functions\n",
        "if(go_cuda==True):\n",
        "    net = net.cuda() #Moving CNN to GPU\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMcFr-cRbGpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# net = net.cuda() #Moving CNN to GPU\n",
        "print(summary(net,(1,128,128)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCCCkcjnzxUo",
        "colab_type": "text"
      },
      "source": [
        "#### Defining **loss function** (Cross-Entropy Loss), **optimization function** (Stochastic Gradient Descent) and **schedule for the optimizaiton** function update (Multi-Step Learning Rate) or **Adam** algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW_OrbAX0QqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss() #Cross-Entropy loss function selected\n",
        "\n",
        "Use_Adam = False #Flag to choose between Stochastic Gradient Descent and Adam optimizer\n",
        "\n",
        "if(Use_Adam==False):\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5, dampening=0, weight_decay=0, nesterov=False) #Stochastic Gradient Descent optimizer with initial learning rate of 0.1\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[15,25,45,55], gamma=0.1, last_epoch=-1) #The schedule for the learning rate will divide the current one by 10 after a number of epochs in milestones\n",
        "    is_schedule = True #Flag must be set to True a scheduler if being used\n",
        "else:\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) #Using Adam algorithm\n",
        "    is_schedule = False #Flag must be set to True a scheduler if being used"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efNs7Ub32Yn0",
        "colab_type": "text"
      },
      "source": [
        "#### **Starting training process**. Script allows for printing the training and validation losses\n",
        "\n",
        "The second cell prints out the graphs for training and validation losses and accuracies.\n",
        "\n",
        "*P.S.1: The loss for the validation dataset is also calculated; however, to prevent leakage of data, it is a different function than the one for the training dataset, which does not receive the instantiated* ```optimizer```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98mCKsEY2adE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################################\n",
        "\"\"\"                GENERAL LIST PROCESS                \"\"\"\n",
        "##########################################################\n",
        "\n",
        "train_loss_list = [] #List to store average training loss for each epoch\n",
        "validation_loss_list = [] #List to store the average validation loss for each epoch\n",
        "\n",
        "accuracy_train = [] #List to store the training accuracy of each epoch\n",
        "accuracy_valid = [] #List to store the validation accuracy of each epoch\n",
        "\n",
        "log = True #Flag for saving the info in a log file\n",
        "if(log==True):\n",
        "    Log_File = []\n",
        "else:\n",
        "    Log_File = None\n",
        "\n",
        "##########################################################\n",
        "\"\"\"               STARTING TRANING EPOCHS              \"\"\"\n",
        "##########################################################\n",
        "\n",
        "num_epochs = 60 #Defining number of epochs to train the model\n",
        "\n",
        "total_start_time = time.time() #Starting clock to measure total training time\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss, temp1 = pf.LossInTraining(NN=net, TrainingLoader=train_loader, Criterion=criterion, Optimizer=optimizer, TrainLength=len(X_train),\n",
        "                                          BatchSize=batchs, Epoch=epoch, is_CNN=Is_CNN, ImageSize=input_size, UseGPU=go_cuda, PrintPartialLoss=True,\n",
        "                                          PartialBatch=15000, log_file=Log_File)\n",
        "    \n",
        "    valid_loss, temp2 = pf.LossInValidation(NN=net, ValidationLoader=valid_loader, Criterion=criterion, ValidLength=len(X_valid),\n",
        "                                            BatchSize=batchs, is_CNN=Is_CNN, ImageSize=input_size, UseGPU=go_cuda)\n",
        "\n",
        "    #Updating lists by adding calculated loss and accuracy values for current epoch\n",
        "    train_loss_list.append(train_loss)\n",
        "    validation_loss_list.append(valid_loss)\n",
        "    accuracy_train.append(sum(temp1)/len(X_train))\n",
        "    accuracy_valid.append(sum(temp2)/len(X_valid))\n",
        "\n",
        "    print('{Epoch %d} - Train loss: %.6f' %(epoch+1, train_loss))\n",
        "    print('{Epoch %d} - Validation loss: %.6f' %(epoch+1, valid_loss))\n",
        "    print('{Epoch %d} - Train accuracy: %.6f' %(epoch+1, accuracy_train[-1]))\n",
        "    print('{Epoch %d} - Validation accuracy: %.6f' %(epoch+1, accuracy_valid[-1]))\n",
        "    \n",
        "    if(log==True): #Adding info to log file\n",
        "        Log_File.append('{Epoch %d} - Train loss: %.6f' %(epoch+1, train_loss))\n",
        "        Log_File.append('{Epoch %d} - Validation loss: %.6f' %(epoch+1, valid_loss))\n",
        "        Log_File.append('{Epoch %d} - Train accuracy: %.6f' %(epoch+1, accuracy_train[-1]))\n",
        "        Log_File.append('{Epoch %d} - Validation accuracy: %.6f' %(epoch+1, accuracy_valid[-1]))\n",
        "    \n",
        "    if(is_schedule==True):\n",
        "        if(epoch<30):\n",
        "            scheduler.step() #Increasing scheduler step\n",
        "        elif(epoch==30):\n",
        "            optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5, dampening=0, weight_decay=0, nesterov=False) #Resetting the Learning Rate\n",
        "        elif(epoch>30):\n",
        "            scheduler.step()\n",
        "\n",
        "    th.cuda.empty_cache()\n",
        "\n",
        "print('Finished training CNN in %0.3f minutes'%((time.time()-total_start_time)/60))\n",
        "\n",
        "if(log==True):\n",
        "    Log_File.append('Finished training CNN in %0.3f minutes'%((time.time()-total_start_time)/60))\n",
        "\n",
        "##########################################################\n",
        "\"\"\"               FINISHED TRANING EPOCHS              \"\"\"\n",
        "##########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOLSnQDvITaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = pd.DataFrame({'Epochs':list(range(num_epochs)),'Training Loss':train_loss_list,'Validation Loss':validation_loss_list})\n",
        "accuracies = pd.DataFrame({'Epochs':list(range(num_epochs)),'Training Accuracy':accuracy_train,'Validation Accuracy':accuracy_valid})\n",
        "\n",
        "pf.Plot_Multi_Curves(Data=losses, Xlabel=\"Epochs\", Ylabel=\"Average Loss\", Title=\"Loss\", Xlim=True, Xlim1=0, Xlim2=(num_epochs+1), Ylim=False, Ylim1=0, Ylim2=100, save=True)\n",
        "\n",
        "pf.Plot_Multi_Curves(Data=accuracies, Xlabel=\"Epochs\", Ylabel=\"Accuracy\", Title=\"Accuracies\", Xlim=True, Xlim1=0, Xlim2=(num_epochs+1), Ylim=True, Ylim1=0.00, Ylim2=1.00, save=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMbZ2xLPPHos",
        "colab_type": "text"
      },
      "source": [
        "#### **Overall training and validation dataset accuracies** after trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cmwl1m6PGPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################################\n",
        "\"\"\"  TRAINING AND VALIDATION ACCURACIES FOR THE MODEL  \"\"\"\n",
        "##########################################################\n",
        "\n",
        "train_set_pred = pf.GetPredsAccur(NeuralNet=net, DataLoader=train_loader, DatasetType='Training', is_CNN=Is_CNN, ImageSize=input_size, UseGPU=go_cuda,\n",
        "                                  PrintAccur=True, GetLebelsPreds=True, List=True, log_file=Log_File)\n",
        "\n",
        "val_set_pred = pf.GetPredsAccur(NeuralNet=net, DataLoader=valid_loader, DatasetType='Validation', is_CNN=Is_CNN, ImageSize=input_size, UseGPU=go_cuda,\n",
        "                                PrintAccur=True, GetLebelsPreds=True, List=True, log_file=Log_File)\n",
        "\n",
        "if(log==True):\n",
        "    Logging = pd.DataFrame({'Log_Info':Log_File})\n",
        "    pf.Write_File_DF(Data_Set=Logging, File_Name=\"log\", separation=\",\", head=True, ind=False) #Printing logging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkHdw5M79qdg",
        "colab_type": "text"
      },
      "source": [
        "#### **Saving the trained CNN model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWO_hxe39p9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(True): #Change boolean value to avoid saving trained model\n",
        "    timestr = time.strftime(\"%y-%m-%d_%Hh%Mm%Ss_\")\n",
        "    path = 'CNN_G23_Std_Model_best.pkl'\n",
        "    th.save(net.state_dict(), timestr+path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8I9245aitXM",
        "colab_type": "text"
      },
      "source": [
        "#### **Uploading saved trained CNN model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqjMN5Dgit3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(False): #Set this flag to True to upload the trained model\n",
        "    net = pf.ConvNN_G23_Std(num_classes=output_size, input_ratio=8, soft_max=False, drop_out=False, drop_prob=0.25, FC4_relu=True)\n",
        "    net.load_state_dict(th.load(\"CNN_G23_Std_Model_best.pkl\"))\n",
        "    net.eval()\n",
        "    net = net.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNs6sUo8PabB",
        "colab_type": "text"
      },
      "source": [
        "#### **Making predictions for the held-out (test) data**\n",
        "\n",
        "*P.S.: to fit the structure of the functions created, the TensorDataset created for the test dataset uses random tensor numbers as label. However, inside the prediction funciton, these \"random labels\" are discarded*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-9CFOkWPa0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################################\n",
        "\"\"\"                 MAKING PREDICITONS                 \"\"\"\n",
        "##########################################################\n",
        "\n",
        "test_dataset = th.utils.data.TensorDataset(Test_Images, th.rand(10000)) #Creating testing dataset by appending random tensor labels to the test dataset for it to be iterable for the prediction function\n",
        "test_loader = th.utils.data.DataLoader(test_dataset, batch_size=batchs, shuffle=False) #Creating test set dataloader with the test_dataset and the batch size specified \n",
        "\n",
        "Results = pf.KagglePreds(NeuralNet=net, DataLoader=test_loader, is_CNN=Is_CNN, ImageSize=input_size, UseGPU=go_cuda, GetLebelsPreds=True) #Predicting\n",
        "\n",
        "pf.Write_File_DF(Data_Set=Results, File_Name=\"Predictions_Group_23\", separation=\",\", head=True, ind=False) #Saving results as .csv file\n",
        "\n",
        "##################################################################################################################################\n",
        "'''                                                              END                                                           '''\n",
        "##################################################################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
